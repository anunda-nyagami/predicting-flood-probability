{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import iqr\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\n\n# Preprocessing\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Options\npd.set_option('display.max_columns',50)\nplt.style.use('bmh')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv', index_col ='id')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv', index_col ='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preview datasets","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = train.drop('FloodProbability', axis = 1).columns.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"for col in cols:\n    fig, ax = plt.subplots(figsize=(6,2))\n    max_val = round(train[col].max()) + 1\n    train[col].hist(density=True,bins = np.arange(0,max_val,1), ax=ax)\n    plt.xticks(np.arange(0,20,1))\n    plt.title(col)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive analysis\nThe data shows that all variables have a median of 5 and a mean of 4.9, with variance and standard deviation nearly identical across the board. There is a moderate right skew in the data distribution.","metadata":{}},{"cell_type":"code","source":"round(train.agg(['min','mean','median','max','var','std','skew']),2).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(test.agg(['min','mean','median','max','var','std','skew']),2).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation\nThere is no correlation between variables","metadata":{}},{"cell_type":"code","source":"corr = train.drop('FloodProbability', axis=1).corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr, mask = mask,linewidth=0.1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr, mask = mask,linewidth=0.1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n## Outliers\nAll variables have outliers","metadata":{}},{"cell_type":"code","source":"train.drop('FloodProbability', axis=1).plot(kind='box',vert=False)\nplt.title('Boxplot of train variables')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.plot(kind='box',vert=False)\nplt.title('Boxplot of test variables')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing outliers","metadata":{}},{"cell_type":"code","source":"for col in cols:\n    col_iqr = iqr(train[col])\n    Q1, Q3 = np.quantile(train[col], [0.25, 0.75])\n    \n    # Convert outliers to np.nan\n    train.loc[train[col] < (Q1 - 1.5*col_iqr), col] = np.nan\n    train.loc[train[col] > (Q3 + 1.5*col_iqr), col] = np.nan    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fraction of outliers is less than 3% in each variable. We can drop the outliers.","metadata":{}},{"cell_type":"code","source":"train.isna().sum()/train.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape before :',train.shape)\ntrain.dropna(how='any', inplace=True)\nprint(\"Shape after :\",train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['FloodProbability']\nX = train.drop('FloodProbability', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After dropping of outliers, the variables are no longer skewed.","metadata":{}},{"cell_type":"code","source":"for col in cols:\n    fig, ax = plt.subplots(figsize=(6,2))\n    max_val = round(X[col].max()) + 2\n    X[col].hist(density=True, bins = np.arange(0,max_val,1), ax=ax)\n    plt.title(col)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:36:34.653969Z","iopub.execute_input":"2024-05-10T07:36:34.654436Z","iopub.status.idle":"2024-05-10T07:36:34.839557Z","shell.execute_reply.started":"2024-05-10T07:36:34.654404Z","shell.execute_reply":"2024-05-10T07:36:34.838549Z"}}},{"cell_type":"code","source":"# Logistic Regression (can output probabilities for binary classification)\nlogistic = LogisticRegression()\n\n# Random Forest Regression\nforest = RandomForestRegressor()\n\n# Decision tree regressor\ndt = DecisionTreeRegressor()\n\n# Gradient Boosting Regression\ngradient = GradientBoostingRegressor()\n\n# Support Vector Regression\nsvr = SVR()\n\n# xgboost\nxgb_reg = xgb.XGBRFRegressor(objective = 'binary:logistic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform train probabilities to log odds\nlog_odds_y_train = np.log(y_train / (1 - y_train)).values\n\n# Transform test probabilities to log odds\nlog_odds_y_test = np.log(y_test / (1 - y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nsteps = [('scaler', scaler),\n        ('xgboost',xgb_reg)]\npipeline = Pipeline(steps)\n\nkf = KFold(n_splits=5, shuffle = True, random_state = 987)\n\n\npipeline.fit(X_train,y_train)\ny_pred_proba = pipeline.predict(X_test)\nscore = (np.round(y_pred_proba)== np.round(y_test)).sum()/len(y_test)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressors = [\n    ('random forest', forest),\n    ('decision tree',dt),\n    ('gradient boosting',gradient),\n    ('support vector', svr),\n    ('xgboost',xgb_reg)\n]\n\nkf = KFold(n_splits=5, shuffle = True, random_state = 987)\n\nfor clf, model in regressors:\n    model.fit(X_train,log_odds_y_train)\n    y_pred_proba = model.predict_proba(X_test)\n    score = (round(y_pred_proba)==round(y_test)).sum()/len(y_test)\n    print(clf, score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}